{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a vanilla Backpropagation algorith\n",
    "\n",
    "In this section, we will learn how to implement the backpropagation algorithm from scratch using Python. \n",
    "\n",
    "**What is Backpropagation?**\n",
    "Back-propagation is the essence of neural net training. It is the method of fine-tuning the weights of a neural net based on the error rate obtained in the previous epoch (i.e., iteration). Proper tuning of the weights allows you to reduce error rates and to make the model reliable by increasing its generalization.\n",
    "\n",
    "Backpropagation is a short form for \"backward propagation of errors.\" It is a standard method of training artificial neural networks. This method helps to calculate the gradient of a loss function with respects to all the weights in the network.\n",
    "\n",
    "The backpropagation algorithm consists of two phases:\n",
    "- 1. The forward pass where we pass our inputs through the network to obtain our output\n",
    "classifications.\n",
    "\n",
    "-  2. The backward pass (i.e., weight update phase) where we compute the gradient of the loss function and use this information to iteratively apply the chain rule to update the weights in our network.\n",
    "\n",
    "\n",
    "<img src=\"images/ex-backpropagation.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid or Logistic Activation Function\n",
    "\n",
    "Neurons activation function of our choose.\n",
    "\n",
    "Sigmoid fn Convert any value to between (0 to 1). In the image below you will see the values transformed to a range of 0-1\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/485/1*Xu7B5y9gp0iL5ooBj7LtWw.png\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\t# sigmoid activation function\n",
    "\tdef sigmoid(dotCalcResult):\n",
    "\t\treturn 1.0 / (1 + np.exp(-dotCalcResult))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why derivative/differentiation is used ?\n",
    "\n",
    "When updating the curve, to know in which direction and how much to change or update the curve depending upon the slope.\n",
    "\n",
    "The error is given by the y-axis. If you’re in point A and want to reduce the error toward 0, then you need to bring the x value down. On the other hand, if you’re in point B and want to reduce the error, then you need to bring the x value up. To know which direction you should go to reduce the error, you’ll use the derivative. \n",
    "\n",
    "<img src=\"https://files.realpython.com/media/quatratic_function.002729dea332.png\" width=\"500px\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The derivative of the sigmoid which we’ll use during the backward pass\n",
    "\n",
    "def sigmoid_deriv(activationFuncResult):\n",
    "\t\treturn activationFuncResult * (1 - activationFuncResult)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning rate\n",
    "\n",
    "Is the step size at each iteration while moving toward a minimum of a loss function. Learning rate can not be too small or too big. \n",
    "\n",
    "\n",
    "<img src=\"https://srdas.github.io/DLBook/DL_images/TNN2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The dot product \n",
    "\n",
    "The dot product of two vectors tells you how similar they are in terms of direction and is scaled by the magnitude of the two vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dot product 1 is: 2.1672\n",
      "The dot product 2 is: 4.1259999999999994\n",
      "The dot product 2 is closer: 4.1259999999999994 > 2.1672\n"
     ]
    }
   ],
   "source": [
    "input_vector = [1.72, 1.23]\n",
    "weights_1 = [1.26, 0]\n",
    "weights_2 = [2.17, 0.32]\n",
    "\n",
    "# Computing the dot product of input_vector and weights_1\n",
    "first_indexes_mult = input_vector[0] * weights_1[0]\n",
    "second_indexes_mult = input_vector[1] * weights_1[1]\n",
    "dot_product_1 = first_indexes_mult + second_indexes_mult\n",
    "\n",
    "print(f\"The dot product 1 is: {dot_product_1}\")\n",
    "#use np instead\n",
    "dot_product_2 = np.dot(input_vector, weights_2)\n",
    "print(f\"The dot product 2 is: {dot_product_2}\")\n",
    "print(f\"The dot product 2 is closer: {dot_product_2} > {dot_product_1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias\n",
    "It sets the result when all the other independent variables are equal to zero. We are using a static value and not changing it during the learning. We are only change the wheights.\n",
    "\n",
    "A bias unit is an \"extra\" neuron added to each pre-output layer that stores the value of 1. Bias units aren't connected to any previous layer and in this sense don't represent a true \"activity\".\n",
    "\n",
    "<a href=\"https://www.quora.com/What-is-bias-in-artificial-neural-network\">More</a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix multiplication\n",
    "\n",
    "The fundamental operations of any typical neural network can be reduced to a bunch of addition and multiplication operations. Neural networks can be expressed in terms of matrices. Matrix multiplication is one of the most important mathematical operations when it comes to deep neural networks.\n",
    "\n",
    "<img src=\"https://storage.ning.com/topology/rest/1.0/file/get/2808330901?profile=original\"/>\n",
    "\n",
    "Below a very simple \"neural net\" for helping to understand matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0]  ->  [[0]]\n",
      "[1, 0]  ->  [[1]]\n",
      "[0, 1]  ->  [[1]]\n",
      "[1, 1]  ->  [[0]]\n"
     ]
    }
   ],
   "source": [
    "#weights for 2 neurons\n",
    "weights_for_layer1 = np.array([\n",
    "\t[1.5, \t-0.5],\n",
    "\t[-1, \t   1],\n",
    "\t[-1, \t   1]\n",
    "])\n",
    "\n",
    "#weights for 1 neurons\n",
    "weights_for_layer2 = np.array([\n",
    "\t[-1],\n",
    "\t[1],\n",
    "\t[1]\n",
    "])\n",
    "\n",
    "def step(x):\n",
    "\treturn np.where(x>0, 1, 0)\n",
    "\n",
    "def neural_net(inputs, layer_weights, activation_function):\n",
    "\toutputs = inputs\n",
    "    #The output of a layer become the input for the subsequent layer\n",
    "\tfor weight in layer_weights:\n",
    "\t\tbias = np.ones(shape=(outputs.shape[0], 1)) #adding 1 as a bias as an extra column\n",
    "\t\tinputs = np.hstack([bias, outputs])\n",
    "\t\tmatrixMultiplied= np.matmul(inputs, weight)\n",
    "\t\toutputs = activation_function(matrixMultiplied)\n",
    "\treturn outputs\n",
    "\n",
    "inputs = [\n",
    "\t[0, 0],\n",
    "\t[1, 0],\n",
    "\t[0, 1],\n",
    "\t[1, 1]\n",
    "]\n",
    "\n",
    "for i in inputs:\n",
    "\tprint(\n",
    "\t\ti,\n",
    "\t\t\" -> \",\n",
    "\t\tneural_net(\n",
    "\t\t\tinputs=np.array([i]),\n",
    "\t\t\tlayer_weights=[weights_for_layer1, weights_for_layer2],\n",
    "\t\t\tactivation_function=step\n",
    "\t\t)\n",
    "\t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(trainDataItem, weights):\n",
    "\t# this list is responsible for storing the output activations for each layer as our data point x\n",
    "\t# forward propagates through the network. We initialize this list with x, which is simply the input data point\n",
    "\tpredictions = [np.atleast_2d(trainDataItem)]\n",
    "\t# loop over the layers in the network\n",
    "\tfor layer in np.arange(0, len(weights)):\n",
    "\t\t# feedforward the activation at the current layer by\n",
    "\t\t# taking the dot product between the activation and the weight matrix -- this is called the \"net input\" to the current layer\n",
    "\t\tnet = predictions[layer].dot(weights[layer])\n",
    "\t\tpredictions.append(sigmoid(net))\n",
    "\treturn predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After each forward pass through a network, backpropagation performs a backward pass while adjusting the model’s parameters (weights and biases)\n",
    "def back_propagation(predictions, label, weights):\n",
    "\t# The first phase of backpropagation is to compute the difference between our *prediction* and the right label\n",
    "\terror = predictions[-1] - label\n",
    "\n",
    "\t# from here, we need to apply the chain rule and build our list of error losses\n",
    "\terror_losses = [error * sigmoid_deriv(predictions[-1])]\n",
    "\n",
    "\t# loop over the layers in reverse order(Back propagation ;) (ignoring the last one since we already calculated it above)\n",
    "\t# -2 because it start from 0\n",
    "\tfor layer in np.arange(len(predictions) - 2, 0, -1):\n",
    "\t\t# calculating the delta diff between the previous delta diff and layer weights\n",
    "\t\terror_loss = error_losses[-1].dot(weights[layer].T)\n",
    "\t\terror_loss = error_loss * sigmoid_deriv(predictions[layer])\n",
    "\t\terror_losses.append(error_loss)\n",
    "\n",
    "\t# since we looped over our layers in reverse order we need to reverse the losses\n",
    "\terror_losses = error_losses[::-1]\n",
    "\treturn error_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "\tdef __init__(self, layers, learning_rate=0.1):\n",
    "\t\tprint(\"[INFO] starting the network with {} layers and  learn rate = {}\".format(len(layers), learning_rate))\n",
    "\t\t# initialize the list of weights matrices\n",
    "\t\tself.weights = []\n",
    "\t\tself.layers = layers\n",
    "\t\tself.learning_rates = learning_rate\n",
    "\t\tself.load_random_weights(layers)\n",
    "\n",
    "    # Loading the initial weights randomically to all layers\n",
    "\tdef load_random_weights(self, layers):\n",
    "\t\t# start looping from the index of the first layer but stop before we reach the last two layers\n",
    "\t\tbias = 1\n",
    "\t\tfor i in np.arange(0, len(layers) - 2):\n",
    "\t\t\tcurrent_layer_neurons = layers[i]\n",
    "\t\t\tnext_layer_neurons = layers[i + 1]\n",
    "\n",
    "\t\t\t# To account for the bias, we add 1 to the number of current and next layer\n",
    "\t\t\t# randomly initialize a weight matrix connecting the number of nodes in each respective layer together,\n",
    "\t\t\trows = current_layer_neurons + bias\n",
    "\t\t\tcolumns = next_layer_neurons + bias\n",
    "\t\t\tw = np.random.randn(rows, columns)\n",
    "\t\t\tself.weights.append(w / np.sqrt(current_layer_neurons))\n",
    "\n",
    "\t\t# the last two layers are a special case where the input connections need a bias term but the output does not\n",
    "\t\tlast_layer = layers[-1]\n",
    "\t\tbefore_last_layer = layers[-2]\n",
    "\n",
    "\t\tw = np.random.randn(before_last_layer + bias, last_layer)\n",
    "\t\tself.weights.append(w / np.sqrt(before_last_layer))\n",
    "\n",
    "\t#  this function is useful for debugging:\n",
    "\tdef __repr__(self):\n",
    "\t\t# construct and return a string that represents the network architecture\n",
    "\t\treturn \"NeuralNetwork: {}\".format(\"-\".join(str(l) for l in self.layers))\n",
    "\n",
    "\tdef update_weights(self, predictions, error_losses):\n",
    "\t\tfor layer in np.arange(0, len(self.weights)):\n",
    "\t\t\t# update our weights by taking the dot product of the layer\n",
    "\t\t\t# activations with their respective losses, then multiplying\n",
    "\t\t\t# this value by some small learning rate and adding to our\n",
    "\t\t\t# weight matrix -- this is where the actual \"learning\" takes place\n",
    "\t\t\tself.weights[layer] += -self.learning_rates * predictions[layer].T.dot(error_losses[layer])\n",
    "\n",
    "\tdef train(self, data_points, labels, epochs=1000, display_update=100):\n",
    "\t\t# insert a column of 1’s as the last entry in the feature matrix -- this little trick allows us to treat the bias\n",
    "\t\t# as a trainable parameter within the weight matrix\n",
    "\t\tdata_points = np.c_[data_points, np.ones((data_points.shape[0]))]\n",
    "\t\tlosses = []\n",
    "\t\t# loop over the desired number of epochs\n",
    "\t\tfor epoch in np.arange(0, epochs):\n",
    "\t\t\t# loop over each individual data point and train our network on it\n",
    "\t\t\tfor (data, label) in zip(data_points, labels):\n",
    "\t\t\t\tpredictions = feed_forward(data, self.weights)\n",
    "\t\t\t\terror_losses = back_propagation(predictions, label, self.weights)\n",
    "\t\t\t\tself.update_weights(predictions, error_losses)\n",
    "\t\t\t\n",
    "\t\t\tif epoch == 0 or (epoch + 1) % 5 == 0:\n",
    "\t\t\t\tloss = self.calculate_loss(data_points, labels)\n",
    "\t\t\t\tlosses.append(loss)\n",
    "\t\t\t# check to see if we should display a training update\n",
    "\t\t\tif epoch == 0 or (epoch + 1) % display_update == 0:\n",
    "\t\t\t\tprint(\"[INFO] epoch={}, loss={:.7f}\".format(epoch + 1, loss))\n",
    "\t\treturn losses\n",
    "\n",
    "\tdef predict(self, data_point, add_bias=True):\n",
    "\t\t# initialize the output prediction as the input features -- this\n",
    "\t\t# value will be (forward) propagated through the network to\n",
    "\t\t# obtain the final prediction\n",
    "\t\tp = np.atleast_2d(data_point)\n",
    "\t\t# check to see if the bias column should be added\n",
    "\t\tif add_bias:\n",
    "\t\t\t# insert a column of 1’s as the last entry in the feature matrix (bias)\n",
    "\t\t\tp = np.c_[p, np.ones((p.shape[0]))]\n",
    "\n",
    "\t\t# loop over our layers in the network\n",
    "\t\tfor layer in np.arange(0, len(self.weights)):\n",
    "\t\t\t# computing the output prediction is as simple as taking\n",
    "\t\t\t# the dot product between the current activation value ‘p‘\n",
    "\t\t\t# and the weight matrix associated with the current layer,\n",
    "\t\t\t# then passing this value through a nonlinear activation\n",
    "\t\t\t# function\n",
    "\t\t\tp = sigmoid(np.dot(p, self.weights[layer]))\n",
    "\t\treturn p\n",
    "\n",
    "\tdef calculate_loss(self, trainItemData, label):\n",
    "\t\tpredictions = self.predict(trainItemData, add_bias=False)\n",
    "\t\tloss = 0.5 * np.sum((predictions - label) ** 2) # MSE cost function\n",
    "\t\treturn loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation with Python Example: MNIST Sample\n",
    "\n",
    "Let’s examine our Neural network with the MNIST dataset for handwritten digit recognition. This subset of the MNIST dataset is built-into the scikit-learn library and includes 1,797 example digits, each of which are 8 × 8 grayscale images (the original images are 28 × 28. When flattened, these images are represented by an 8 × 8 = 64-dim vector.\n",
    "\n",
    "![alt text](images/mnist-sample.png \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading MNIST (sample) dataset...\n",
      "[INFO] samples: 1797, dim: 64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import datasets\n",
    "\n",
    "# load the MNIST dataset and apply min/max scaling to scale the\n",
    "# pixel intensity values to the range [0, 1] (each image is\n",
    "# represented by an 8 x 8 = 64-dim feature vector)\n",
    "print(\"[INFO] loading MNIST (sample) dataset...\")\n",
    "digits = datasets.load_digits()\n",
    "data = digits.data.astype(\"float\")\n",
    "data = (data - data.min()) / (data.max() - data.min())\n",
    "print(\"[INFO] samples: {}, dim: {}\".format(data.shape[0],data.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 3, 9, 3, 7, 9, 6, 0, 8, 7, 5, 5, 0, 1, 4, 6, 9, 5, 5, 2, 1, 3,\n",
       "       4, 4, 2, 9, 3, 6, 3, 7, 4, 8, 7, 5, 6, 9, 0, 2, 2, 1, 2, 8, 7, 6,\n",
       "       3, 5, 1, 6, 1, 9, 6, 9, 9, 2, 6, 3, 3, 4, 1, 5, 3, 8, 1, 6, 4, 0,\n",
       "       6, 6, 7, 6, 6, 7, 0, 9, 8, 0, 2, 8, 0, 2, 2, 3, 5, 6, 1, 0, 6, 6,\n",
       "       9, 9, 7, 8, 3, 9, 9, 8, 4, 5, 8, 3, 9, 6, 9, 6, 0, 7, 2, 3, 6, 8,\n",
       "       3, 1, 3, 7, 0, 7, 9, 4, 6, 4, 1, 4, 3, 7, 7, 4, 4, 5, 5, 0, 8, 5,\n",
       "       4, 7, 5, 8, 0, 8, 1, 1, 8, 5, 0, 4, 4, 9, 8, 4, 4, 4, 2, 8, 7, 6,\n",
       "       6, 0, 3, 5, 0, 1, 5, 2, 0, 2, 4, 6, 4, 8, 8, 0, 0, 0, 9, 3, 5, 7,\n",
       "       0, 0, 1, 9, 8, 8, 2, 6, 5, 9, 4, 1, 9, 4, 5, 7, 2, 3, 5, 4, 1, 2,\n",
       "       0, 0, 2, 4, 4, 7, 3, 3, 2, 7, 7, 9, 0, 5, 5, 9, 0, 4, 7, 2, 6, 2,\n",
       "       1, 9, 3, 1, 5, 7, 3, 1, 0, 6, 2, 9, 0, 3, 6, 0, 7, 4, 8, 5, 0, 6,\n",
       "       4, 5, 0, 5, 0, 6, 8, 2, 9, 8, 0, 1, 3, 7, 8, 2, 1, 9, 4, 4, 8, 1,\n",
       "       4, 1, 1, 5, 0, 5, 1, 3, 8, 2, 9, 0, 7, 7, 6, 1, 3, 5, 1, 5, 7, 9,\n",
       "       8, 9, 4, 3, 1, 2, 3, 3, 9, 9, 7, 0, 2, 9, 5, 7, 7, 5, 8, 5, 8, 7,\n",
       "       9, 0, 7, 1, 0, 6, 9, 8, 3, 1, 1, 0, 8, 3, 8, 9, 7, 9, 1, 7, 8, 1,\n",
       "       4, 7, 2, 5, 6, 4, 4, 5, 1, 6, 3, 9, 3, 2, 5, 0, 9, 1, 1, 4, 6, 5,\n",
       "       1, 9, 5, 3, 3, 9, 6, 5, 4, 6, 4, 3, 8, 9, 3, 2, 5, 3, 9, 4, 6, 5,\n",
       "       0, 7, 2, 7, 1, 0, 2, 9, 7, 1, 7, 2, 7, 3, 5, 8, 8, 6, 7, 2, 3, 3,\n",
       "       9, 0, 9, 0, 6, 1, 0, 3, 6, 0, 3, 6, 9, 2, 3, 2, 4, 5, 3, 2, 4, 8,\n",
       "       8, 0, 6, 5, 5, 6, 4, 8, 7, 4, 5, 9, 3, 0, 3, 5, 2, 4, 1, 0, 6, 7,\n",
       "       9, 3, 8, 1, 6, 5, 6, 0, 7, 6])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# construct the training and testing splits\n",
    "(trainData, testData, trainLabel, testLabel) = train_test_split(data, digits.target, test_size=0.25)\n",
    "testLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Binarize the labels. convert the labels from integers to vectors, tranform in a list with 0s and 1\n",
    "trainLabel = LabelBinarizer().fit_transform(trainLabel)\n",
    "testLabel = LabelBinarizer().fit_transform(testLabel)\n",
    "testLabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training network...\n",
      "[INFO] starting the network with 4 layers and  learn rate = 0.1\n",
      "[INFO] NeuralNetwork: 64-32-16-10\n",
      "[INFO] epoch=1, loss=605.9826960\n",
      "[INFO] epoch=100, loss=8.1513370\n",
      "[INFO] epoch=200, loss=2.1584333\n",
      "[INFO] epoch=300, loss=1.5697827\n",
      "[INFO] epoch=400, loss=1.3691824\n",
      "[INFO] epoch=500, loss=1.2637834\n",
      "[INFO] epoch=600, loss=0.7530873\n",
      "[INFO] epoch=700, loss=0.6916449\n",
      "[INFO] epoch=800, loss=0.6564577\n",
      "[INFO] epoch=900, loss=0.6324447\n",
      "[INFO] epoch=1000, loss=0.6148131\n"
     ]
    }
   ],
   "source": [
    "# train the network\n",
    "print(\"[INFO] training network...\")\n",
    "# number of nodes for each layer, the last one is the number of digits(options)\n",
    "nn = NeuralNetwork([trainData.shape[1], 32, 16, 10])\n",
    "\n",
    "print(\"[INFO] {}\".format(nn))\n",
    "losses = nn.train(trainData, trainLabel, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-13-02c04f617f7c>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mmatplotlib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpyplot\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mplt\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mplot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlosses\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mxlabel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Iterations\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mylabel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Error for all training instances\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'losses' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(losses)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Error for all training instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = nn.predict(testData)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions.argmax(axis=1)\n",
    "print(classification_report(testLabel.argmax(axis=1), predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the precision column and you will see the percentage of right prediction to each number. The closer to 1 the best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[np.atleast_2d([2,3])][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (\"John\", \"Charles\", \"Mike\")\n",
    "b = (\"Jenny\", \"Christy\", \"Monica\", \"Vicky\")\n",
    "x = zip(a, b)\n",
    "\n",
    "#use the tuple() function to display a readable version of the result:\n",
    "tuple(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randn??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randn(2, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randn(2, 4) / np.sqrt(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.c_??\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}