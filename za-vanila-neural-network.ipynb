{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a vanilla Backpropagation algorith\n",
    "\n",
    "In this section, we will learn how to implement the backpropagation algorithm from scratch using Python. \n",
    "\n",
    "**What is Backpropagation?**\n",
    "Back-propagation is the essence of neural net training. It is the method of fine-tuning the weights of a neural net based on the error rate obtained in the previous epoch (i.e., iteration). Proper tuning of the weights allows you to reduce error rates and to make the model reliable by increasing its generalization.\n",
    "\n",
    "Backpropagation is a short form for \"backward propagation of errors.\" It is a standard method of training artificial neural networks. This method helps to calculate the gradient of a loss function with respects to all the weights in the network.\n",
    "\n",
    "The backpropagation algorithm consists of two phases:\n",
    "- 1. The forward pass where we pass our inputs through the network to obtain our output\n",
    "classifications.\n",
    "\n",
    "-  2. The backward pass (i.e., weight update phase) where we compute the gradient of the loss function and use this information to iteratively apply the chain rule to update the weights in our network.\n",
    "\n",
    "\n",
    "<img src=\"images/ex-backpropagation.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "\tdef __init__(self, layers, alpha=0.1):\n",
    "\t\t# initialize the list of weights matrices\n",
    "\t\tself.weights = []\n",
    "\n",
    "\t\t# layers architecture\n",
    "\t\tself.layers = layers\n",
    "\n",
    "\t\t# learning rate\n",
    "\t\tself.learning_rates = alpha\n",
    "        \n",
    "\t\tself.load_random_weights(layers)\n",
    "\n",
    "    # Loading the initial weights randomically to all layers\n",
    "\tdef load_random_weights(self, layers):\n",
    "\t\t# start looping from the index of the first layer but stop before we reach the last two layers\n",
    "\t\tfor i in np.arange(0, len(layers) - 2):\n",
    "\t\t\tcurrent_layer = layers[i]\n",
    "\t\t\tnext_layer = layers[i + 1]\n",
    "\n",
    "\t\t\t# To account for the bias, we add 1 to the number of current and next layer\n",
    "\t\t\t# randomly initialize a weight matrix connecting the number of nodes in each respective layer together,\n",
    "\t\t\tw = np.random.randn(current_layer + 1, next_layer + 1)\n",
    "\t\t\tself.weights.append(w / np.sqrt(layers[i]))\n",
    "\n",
    "\t\t# the last two layers are a special case where the input connections need a bias term but the output does not\n",
    "\t\tlast_layer = layers[-1]\n",
    "\t\tbefore_last_layer = layers[-2]\n",
    "\n",
    "\t\tw = np.random.randn(before_last_layer + 1, last_layer)\n",
    "\t\tself.weights.append(w / np.sqrt(before_last_layer))\n",
    "\n",
    "\t#  this function is useful for debugging:\n",
    "\tdef __repr__(self):\n",
    "\t\t# construct and return a string that represents the network architecture\n",
    "\t\treturn \"NeuralNetwork: {}\".format(\"-\".join(str(l) for l in self.layers))\n",
    "\n",
    "\t# sigmoid activation function\n",
    "\tdef sigmoid(self, x):\n",
    "\t\treturn 1.0 / (1 + np.exp(-x))\n",
    "\n",
    "\t# the derivative of the sigmoid which we’ll use during the backward pass\n",
    "\tdef sigmoid_deriv(self, x):\n",
    "\t\treturn x * (1 - x)\n",
    "\n",
    "\tdef update_weights(self, activations, deltas):\n",
    "\t\tfor layer in np.arange(0, len(self.weights)):\n",
    "\t\t\t# update our weights by taking the dot product of the layer\n",
    "\t\t\t# activations with their respective deltas, then multiplying\n",
    "\t\t\t# this value by some small learning rate and adding to our\n",
    "\t\t\t# weight matrix -- this is where the actual \"learning\" takes place\n",
    "\t\t\tself.weights[layer] += -self.learning_rates * activations[layer].T.dot(deltas[layer])\n",
    "\n",
    "\tdef back_propagation(self, activations, y):\n",
    "\t\t# the first phase of backpropagation is to compute the difference between our *prediction* (the final output\n",
    "\t\t# activation in the activations list) and the true target value\n",
    "\t\t# Since the final entry in the activations list activations contains the output of the network,\n",
    "\t\t# we can access the output prediction via activations[-1]. The value y is the target output for the input data point x\n",
    "\t\terror = activations[-1] - y\n",
    "\t\t# from here, we need to apply the chain rule and build our list of deltas ‘deltas‘;\n",
    "\t\t# the first entry in the deltas is  simply the error of the output layer times the derivative\n",
    "\t\t# of our activation function for the output value\n",
    "\t\tdeltas = [error * self.sigmoid_deriv(activations[-1])]\n",
    "\t\t# loop over the layers in reverse order (ignoring the last two since we already have taken them into account)\n",
    "\t\tfor layer in np.arange(len(activations) - 2, 0, -1):\n",
    "\t\t\t# the delta for the current layer is equal to the delta of the *previous layer* dotted with the weight matrix\n",
    "\t\t\t# of the current layer, followed by multiplying the delta by the derivative of the nonlinear activation function\n",
    "\t\t\t# for the activations of the current layer\n",
    "\t\t\tdelta = deltas[-1].dot(self.weights[layer].T)\n",
    "\t\t\tdelta = delta * self.sigmoid_deriv(activations[layer])\n",
    "\t\t\tdeltas.append(delta)\n",
    "\t\t# since we looped over our layers in reverse order we need to reverse the deltas\n",
    "\t\tdeltas = deltas[::-1]\n",
    "\t\treturn deltas\n",
    "\n",
    "\tdef feed_forward(self, x):\n",
    "\t\t# this list is responsible for storing the output activations for each layer as our data point x\n",
    "\t\t# forward propagates through the network. We initialize this list with x, which is simply the input data point\n",
    "\t\tactivations = [np.atleast_2d(x)]\n",
    "\t\t# FEEDFORWARD:\n",
    "\t\t# loop over the layers in the network\n",
    "\t\tfor layer in np.arange(0, len(self.weights)):\n",
    "\t\t\t# feedforward the activation at the current layer by\n",
    "\t\t\t# taking the dot product between the activation and the weight matrix -- this is called the \"net input\"\n",
    "\t\t\t# to the current layer\n",
    "\t\t\tnet = activations[layer].dot(self.weights[layer])\n",
    "\t\t\t# computing the \"net output\" is simply applying our nonlinear activation function to the net input\n",
    "\t\t\tout = self.sigmoid(net)\n",
    "            \n",
    "\t\t\t# once we have the net output, add it to our list of activations\n",
    "\t\t\tactivations.append(out)\n",
    "\t\treturn activations\n",
    "\n",
    "\tdef fit(self, data_points, labels, epochs=1000, display_update=100):\n",
    "\t\t# insert a column of 1’s as the last entry in the feature matrix -- this little trick allows us to treat the bias\n",
    "\t\t# as a trainable parameter within the weight matrix\n",
    "\t\tdata_points = np.c_[data_points, np.ones((data_points.shape[0]))]\n",
    "\n",
    "\t\t# loop over the desired number of epochs\n",
    "\t\tfor epoch in np.arange(0, epochs):\n",
    "\t\t\t# loop over each individual data point and train our network on it\n",
    "\t\t\tfor (x, target) in zip(data_points, labels):\n",
    "\t\t\t\tactivations = self.feed_forward(x)\n",
    "\t\t\t\tdeltas = self.back_propagation(activations, target)\n",
    "\t\t\t\tself.update_weights(activations, deltas)\n",
    "\n",
    "\t\t\t# check to see if we should display a training update\n",
    "\t\t\tif epoch == 0 or (epoch + 1) % display_update == 0:\n",
    "\t\t\t\tloss = self.calculate_loss(data_points, labels)\n",
    "\t\t\t\tprint(\"[INFO] epoch={}, loss={:.7f}\".format(epoch + 1, loss))\n",
    "\n",
    "\tdef predict(self, data_point, add_bias=True):\n",
    "\t\t# initialize the output prediction as the input features -- this\n",
    "\t\t# value will be (forward) propagated through the network to\n",
    "\t\t# obtain the final prediction\n",
    "\t\tp = np.atleast_2d(data_point)\n",
    "\t\t# check to see if the bias column should be added\n",
    "\t\tif add_bias:\n",
    "\t\t\t# insert a column of 1’s as the last entry in the feature matrix (bias)\n",
    "\t\t\tp = np.c_[p, np.ones((p.shape[0]))]\n",
    "\n",
    "\t\t# loop over our layers in the network\n",
    "\t\tfor layer in np.arange(0, len(self.weights)):\n",
    "\t\t\t# computing the output prediction is as simple as taking\n",
    "\t\t\t# the dot product between the current activation value ‘p‘\n",
    "\t\t\t# and the weight matrix associated with the current layer,\n",
    "\t\t\t# then passing this value through a nonlinear activation\n",
    "\t\t\t# function\n",
    "\t\t\tp = self.sigmoid(np.dot(p, self.weights[layer]))\n",
    "\n",
    "\t\t# return the predicted value\n",
    "\t\treturn p\n",
    "\n",
    "\tdef calculate_loss(self, X, targets):\n",
    "\t\t# make predictions for the input data points then compute the loss\n",
    "\t\ttargets = np.atleast_2d(targets)\n",
    "\t\tpredictions = self.predict(X, add_bias=False)\n",
    "\t\tloss = 0.5 * np.sum((predictions - targets) ** 2)\n",
    "\t\treturn loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation with Python Example: MNIST Sample\n",
    "\n",
    "Let’s examine our Neural network with the MNIST dataset for handwritten digit recognition. This subset of the MNIST dataset is built-into the scikit-learn library and includes 1,797 example digits, each of which are 8 × 8 grayscale images (the original images are 28 × 28. When flattened, these images are represented by an 8 × 8 = 64-dim vector.\n",
    "\n",
    "![alt text](images/mnist-sample.png \"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading MNIST (sample) dataset...\n",
      "[INFO] samples: 1797, dim: 64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import datasets\n",
    "\n",
    "# load the MNIST dataset and apply min/max scaling to scale the\n",
    "# pixel intensity values to the range [0, 1] (each image is\n",
    "# represented by an 8 x 8 = 64-dim feature vector)\n",
    "print(\"[INFO] loading MNIST (sample) dataset...\")\n",
    "digits = datasets.load_digits()\n",
    "data = digits.data.astype(\"float\")\n",
    "data = (data - data.min()) / (data.max() - data.min())\n",
    "print(\"[INFO] samples: {}, dim: {}\".format(data.shape[0],data.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training network...\n",
      "[INFO] NeuralNetwork: 64-32-16-10\n",
      "[INFO] epoch=1, loss=605.1492535\n",
      "[INFO] epoch=100, loss=6.2829649\n",
      "[INFO] epoch=200, loss=2.0180309\n",
      "[INFO] epoch=300, loss=1.0428557\n",
      "[INFO] epoch=400, loss=0.8497941\n",
      "[INFO] epoch=500, loss=0.7561554\n",
      "[INFO] epoch=600, loss=0.7008680\n",
      "[INFO] epoch=700, loss=0.6644783\n",
      "[INFO] epoch=800, loss=0.6387617\n",
      "[INFO] epoch=900, loss=0.6196323\n",
      "[INFO] epoch=1000, loss=0.6048176\n"
     ]
    }
   ],
   "source": [
    "# construct the training and testing splits\n",
    "(trainX, testX, trainY, testY) = train_test_split(data,digits.target, test_size=0.25)\n",
    "\n",
    "# convert the labels from integers to vectors\n",
    "trainY = LabelBinarizer().fit_transform(trainY)\n",
    "testY = LabelBinarizer().fit_transform(testY)\n",
    "\n",
    "# train the network\n",
    "print(\"[INFO] training network...\")\n",
    "nn = NeuralNetwork([trainX.shape[1], 32, 16, 10])\n",
    "\n",
    "print(\"[INFO] {}\".format(nn))\n",
    "nn.fit(trainX, trainY, epochs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating network...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99        50\n",
      "           1       0.98      0.96      0.97        53\n",
      "           2       1.00      1.00      1.00        55\n",
      "           3       0.98      0.95      0.97        44\n",
      "           4       0.94      1.00      0.97        45\n",
      "           5       0.93      0.98      0.96        44\n",
      "           6       1.00      1.00      1.00        45\n",
      "           7       1.00      1.00      1.00        44\n",
      "           8       0.97      0.88      0.92        32\n",
      "           9       0.95      0.97      0.96        38\n",
      "\n",
      "    accuracy                           0.98       450\n",
      "   macro avg       0.97      0.97      0.97       450\n",
      "weighted avg       0.98      0.98      0.98       450\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = nn.predict(testX)\n",
    "predictions = predictions.argmax(axis=1)\n",
    "print(classification_report(testY.argmax(axis=1), predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the precision column and you will see the percentage of right prediction to each number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
