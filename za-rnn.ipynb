{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)\n",
    "BSD License\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "# data I/O\n",
    "data = open('texts/input.txt', 'r').read() # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print ('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "# hyperparameters\n",
    "hidden_size = 100 # size of hidden layer of neurons\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "learning_rate = 1e-1\n",
    "\n",
    "#Here’s what makes a RNN recurrent: it uses the same weights for each step. More specifically, \n",
    "#a typical vanilla RNN uses only 3 sets of weights to perform its calculations:\n",
    "#Wxh​, used for all x_t​ → h_t​ links.\n",
    "#Whh​, used for all h_{t-1}​ → h_t​ links.\n",
    "#Why​, used for all h_t​ → y_t​ links.\n",
    "\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "\n",
    "\n",
    "#We’ll also use two biases for our RNN:\n",
    "#b_h, added when calculating h_t.\n",
    "#b_y, added when calculating y_t.\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the format of output required to calculate the loss should be one-hot encoded vectors.\n",
    "<img src=\"https://miro.medium.com/max/684/1*JXpSMnwL7Hx1cGny310DYg.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the result in probabilities for the next chars\n",
    "def softmax(xs):\n",
    "\treturn np.exp(xs) / np.sum(np.exp(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The np.tanh function implements a non-linearity that squashes the activations to the range [-1, 1]\n",
    "#Notice briefly how this works: There are two terms inside of the tanh: one is based on the previous hidden state and one is based on the current input. \n",
    "#In numpy np.dot is matrix multiplication. The two intermediates interact with addition, and then get squashed by the tanh into the new state vector. \n",
    "#If you’re more comfortable with math notation, we can also write the hidden state update as ht=tanh(Whhht−1+Wxhxt), where tanh is applied elementwise.\n",
    "def tanh_activation(input_to_hidden_weight, previous_hidden_state, hidden_to_hidden_weight, current_hidden_state,  bh):\n",
    "\treturn np.tanh(np.dot(input_to_hidden_weight, previous_hidden_state) + np.dot(hidden_to_hidden_weight, current_hidden_state) + bh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(ps, target):\n",
    "\treturn -np.log(ps[target,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The forward pass use the parameters of the model (Wxh, Whh, Why, bh, by) to calculate the next char given a char from the trainning set.\n",
    "def forward(inputs, targets, hprev):\n",
    "\txs, hs, ys, ps = {}, {}, {}, {}\n",
    "\ths[-1] = np.copy(hprev)\n",
    "\n",
    "\tfor t in range(len(inputs)):\n",
    "\t\t#xs[t] is the vector that encode the char at position t \n",
    "\t\txs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "\t\txs[t][inputs[t]] = 1\n",
    "\t\tactivation = tanh_activation(Wxh, xs[t],Whh, hs[t-1], bh) # hidden state\n",
    "\t\t# taking the dot product between the activation and the weight matrix -- this is called the \"net input\" to the current layer\n",
    "\t\tnet = np.dot(Why, activation) + by\n",
    "\t\tprediction = softmax(net) # probabilities for next chars\n",
    "\t\ths[t] = activation\n",
    "\t\tys[t] = net\n",
    "\t\tps[t] = prediction\n",
    "\treturn xs, hs, ys, ps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_deriv(activationFuncResult, dh):\n",
    "\treturn (1 - activationFuncResult * activationFuncResult) * dh "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(inputs, targets, activations, predictions, xs):\n",
    "\t# backward pass: compute gradients going backwards\n",
    "\tdWxh = np.zeros_like(Wxh)\n",
    "\tdWhh = np.zeros_like(Whh)\n",
    "\tdWhy = np.zeros_like(Why)\n",
    "\tdbh = np.zeros_like(bh)\n",
    "\tdby = np.zeros_like(by)\n",
    "\tdhnext = np.zeros_like(activations[0])\n",
    "\n",
    "\tfor t in reversed(range(len(inputs))):\n",
    "\t\tprediction = np.copy(predictions[t])\n",
    "\t\tprediction[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "\t\tdWhy += np.dot(prediction, activations[t].T)\n",
    "\t\tdh = np.dot(Why.T, prediction) + dhnext # backprop into h\n",
    "\t\tdirection = tanh_deriv(activations[t], dh) # backprop through tanh nonlinearity\n",
    "\t\tdWxh += np.dot(direction, xs[t].T)\n",
    "\t\tdWhh += np.dot(direction, activations[t-1].T)\n",
    "\t\tdhnext = np.dot(Whh.T, direction)\n",
    "\t\tdby += prediction    \n",
    "\t\tdbh += direction\n",
    "    \n",
    "\tfor dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "\t\tnp.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients\n",
    "    \n",
    "\treturn  dWxh, dWhh, dWhy, dbh, dby, activations[len(inputs)-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(previous_hidden_state, start_index, text_length):\n",
    "\t\"\"\" \n",
    "\tsample a sequence of integers from the model \n",
    "\th is memory state, seed_ix is seed letter for first time step\n",
    "\t\"\"\"\n",
    "\tx = np.zeros((vocab_size, 1))\n",
    "\tx[start_index] = 1\n",
    "\tindexes = []\n",
    "\tfor t in range(text_length):\n",
    "\t\th = np.tanh(np.dot(Wxh, x) + np.dot(Whh, previous_hidden_state) + bh)\n",
    "\t\ty = np.dot(Why, previous_hidden_state) + by\n",
    "\t\tp = np.exp(y) / np.sum(np.exp(y))\n",
    "\t\tindex = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "\t\tx = np.zeros((vocab_size, 1))\n",
    "\t\tx[index] = 1\n",
    "\t\tindexes.append(index)\n",
    "\treturn indexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ntp  htacte   lp Ccwsscgrs  bsd ttie ettisnat  e etsc a et li  ethiepetae  cev tt yu.lityticplia?y  lratitidspe a tsd dse di  yes  bigtsitoyresyab esk !htiRatad rtc rtttbttti csdekepcil te bs s dtwnye'"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_ix = sample(hprev, inputs[0], 200)\n",
    "''.join(ix_to_char[ix] for ix in sample_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "epochs = 500000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform parameter(weights) update with Adagrad\n",
    "def update_weights(dWxh, dWhh, dWhy, dbh, dby):\n",
    "\tfor weights, dWeights, memWeights in zip([Wxh, Whh, Why, bh, by], [dWxh, dWhh, dWhy, dbh, dby], [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "\t\tmemWeights += dWeights * dWeights\n",
    "\t\tweights += -learning_rate * dWeights / np.sqrt(memWeights + 1e-8) # adagrad update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(ps, targets):\n",
    "\tloss = 0\n",
    "\tfor t in range(len(inputs)):\n",
    "\t\tloss += cross_entropy_loss(ps[t],targets[t])\n",
    "        \n",
    "\treturn smooth_loss * 0.999 + loss * 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "    .ue   s e. es ,s    shsae  s ne e es.h e  .s  esuh e  h,ai u   .sshrh h.\n",
      "e is  iy    e.ehek  s h   .lh\n",
      " e     esh eas se e . ycsS.c ehi;si  u eeieh.e .oe  yee,e,ees i   eli siiiRu eeee, sahly ee. h \n",
      "----\n",
      "iter 0, loss: 55.491125\n",
      "----\n",
      " ppSupwwtnwpwtpnpnwwwnnpidnnwkpnpwmwwwwpiidwimptwpnppwrmwtpnvwdpdwpdwintwUsddpc vtmnnpypmepwpintpicdelwtnipppvpinwPpipwuwbnwttnRnwunwtwntnwtpinwwppwwtipvwwrpwdwnwwtpputwmi.wmwwwpnwpttdpwpwnntpwppinwwng \n",
      "----\n",
      "iter 100, loss: 55.426974\n",
      "----\n",
      " iith.hy.i i.  i.top     hy o o.. . ehhth i h,. .tt.     . i  thtthit..i. tht.eh th   .t  a.  h ..h . ih.  t .i h i y  .i  t.h h tt    t h,hh .ht t .   ipi .ij ...i.i t  i t.httt   t .. ti.hh  t  tt    \n",
      "----\n",
      "iter 200, loss: 55.463700\n",
      "----\n",
      " tchi .t                  hi       h  ti         t     h t         t h  .         t t    c         t   t   i. h        , tc      i    t    .  t   i         h ii t       . t     h   t              t     \n",
      "----\n",
      "iter 300, loss: 55.500530\n",
      "----\n",
      " ItatlrUlbPtlwt(ottcngobts sbpatdgtt.tPsxt,ltccttwdaUaep?vtubpr?almyavrbcndrnaittndtcptlncdrardbtcdtatcdnkytittncmbalyasrnfcbl gsprblstvprnpte connpcrttfglpcydtdastcltcavpweasbtcnwsllttcardtalnrgoire,t \n",
      "----\n",
      "iter 400, loss: 55.460582\n",
      "----\n",
      " cnrnhffddgw lgtdvrdu ntdnnfnrvnduddtnnrnlfnnnurntrpnmfdnsnnun fdunnsnnnnn ornnnuunrnmnonnldnlnrnddnvnndnnnd tunudtnnmtuddnnndffgnngfftcdmnmsdhhnnlnorlnvtnnfndnodmdnmnhnfmdnmnntnmndmmhrnnnfnrffdgmnratd \n",
      "----\n",
      "iter 500, loss: 55.398360\n",
      "----\n",
      " ho hihkehoh,ese.hhohth hhehhaiooeoat eeht oh !isoe ph .ihshethhhhlehthh   huhhk's hhhhe uhaeliot totehheetiyehh ei hy ohl o thhistshe soihhois wohoh h  ou  ok.aieoehihst, hhhiteeoesh.isostoethioh  iht \n",
      "----\n",
      "iter 600, loss: 55.348125\n",
      "----\n",
      "  IT e P pceap cTc ieCe ei ps.ceg   iise nt a tc/sspe  s   e ce ep o esei e sar  c  st  e ppiu   i eewiceenpe  epcceP cC  eei.teee   gn     ip  .  s  esf iiigsie a apse Pie. ii y e  ce ee Pp.  e   t ce \n",
      "----\n",
      "iter 700, loss: 55.326335\n",
      "----\n",
      " gmcsoaidrgccrciityIslwsraUeaaevntttstietagCmgitoaiaaadposcsrittalytTtagtcyncu\n",
      "ncmoctwnmabgdsitiiSstiaoocsTscrecccttotcbeCcessIiitfpmtttpovct ativaateciftnllaoTrwpTtiaetcmTssbaasicc\n",
      "wvooatYattctlcvttcc \n",
      "----\n",
      "iter 800, loss: 55.363648\n",
      "----\n",
      " soctdnnspnn tus tsprsttensmmrssncmfsnpncsmfnr mmsmrpvprnnrl rnspsn csmr npmnfmt ns ssnmcgtnnrdrmpsslcncrsn srrwgnsr rsfrprscncrdn nrsdranlnpwnnrnnrgrarsmscsnrrnm pttfdspdsrsrncscsgmlshBcnspffsrmstftn  \n",
      "----\n",
      "iter 900, loss: 55.374008\n",
      "----\n",
      " eee eeteh hi ee ore ol ,e eoe ehyuu eeee  aoe hRe he    e eeee ahooi ohaieai eya heaye y yee ehhhto eeeoee e i h ei oithoeee,ueao hae eooeeo l ed  -eyhooa  oeae aeho aae esueeee  k ,e y ehieihy ey ieh \n",
      "----\n",
      "iter 1000, loss: 55.290201\n",
      "----\n",
      " ofnnnnrofdorfd  nnfnnunfhdngdvnffvnfrnnfdddnvnlnnlnrfnunfndgndnntnnnfgdfnrdnnugsnffdpnd rtfrgrtnndnbdofndudtuuudnnnmgfundtnnunnputnfusuffnfonrfdrupfnfffnfdfnudddf nnnndnnudnudvruomnuuuuntrdunngd nlfnh \n",
      "----\n",
      "iter 1100, loss: 55.295029\n",
      "----\n",
      " ai   e a      s    s    o   es  i   , ,   oas.    r          p.  i       te t.  so         . oe   ee i            i    t    e    i     a s ci  eo i e   . i    sesei . eato  y  e  ?ii i  iayi   i ia    \n",
      "----\n",
      "iter 1200, loss: 55.210084\n",
      "----\n",
      " ruaadCs? daabfest sro,unsascrdddt ppss rsr cc  u cs cmes,trnrdtd  ss l crsvtptkpnc..cerr.srgps se gsc,dedcisasacsad.s pei ci  nr nylu,nppcic  t.rppusa rsursasccs duc sp d   ?u ctIdc obgpianicpi ipeue  \n",
      "----\n",
      "iter 1300, loss: 55.210274\n",
      "----\n",
      " eaeoe,oeee ogec iaa aaaa eareoeeeaeoieee eeia y e aeaaeeoiasea eea  oeaie  aroroeaaaa hoaehTeoeae  o uoaaiee eiaie, eaeey hierdeaeoaeape ae  eaereihee eieeeeeae eeheee eeeee-iaeoe oeaeihia lae ey eaee \n",
      "----\n",
      "iter 1400, loss: 55.243036\n",
      "----\n",
      "  t  siRs   Rt is .ithsht .o i  .  lys ouaat.i it iiit o a.  .saattois. h u i.sh. tcse .ttthis t.e ,sh te    . t i   i   tsstrRu h. i.hu itc  tts osa   a.h st. s ,  p y.ii tpohtiyh .tski i uioSt  a,ths \n",
      "----\n",
      "iter 1500, loss: 55.220793\n"
     ]
    }
   ],
   "source": [
    "for n in np.arange(0, epochs):\n",
    "\t# prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "\tif p+seq_length+1 >= len(data) or n == 0: \n",
    "\t\thprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "\t\tp = 0 # go from start of data\n",
    "\n",
    "\tinputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "\ttargets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\txs, hs, ys, ps = forward(inputs, targets, hprev)\n",
    "\tdWxh, dWhh, dWhy, dbh, dby, hprev = backprop(inputs, targets, hs, ps, xs)    \n",
    "\tsmooth_loss = calculate_loss(ps, targets)\n",
    "\tupdate_weights(dWxh, dWhh, dWhy, dbh, dby)\n",
    "\tp += seq_length # move data pointer\n",
    "\t# sample from the model now and then\n",
    "\tif n % 100 == 0:\n",
    "\t\tsample_ix = sample(hprev, inputs[0], 200)\n",
    "\t\ttxt = ''.join(ix_to_char[ix] for ix in sample_ix)\n",
    "\t\tprint ('----\\n %s \\n----' % (txt, ))\n",
    "\t\tprint ('iter %d, loss: %f' % (n, smooth_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
